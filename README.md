
# Medium Articles Text Generator: An attempt at feature visualization

## Project Overview
Inspired by computer Vision: extraction of features from RNNs (LSTM) and transformers to answer the question of whether such models are sufficient for text generation, returning grammatically correct sentences with coherent meanings; we also wanted to find out what features can be extracted from models and how they compare between LSTM and transformers. We first have to trained a recurrent neural network (LSTM variant of an RNN) and a transformer as examples of NLP models before visualizing the activation of neurons following the input of example sentences to explore how the model represents natural language. 
Our hypothesiz is that later layers may have higher-level representations compared to the earlier layer.
Neuromatch Academy project in Long Short Term Memory (Natural Language Processing). An unfinished attempt to design a sentence generator using elements from pytorch, tensorflow (keras), spacy, word2vec. and then visualizing the activation of neurons to explore how the model represents natural language. 

